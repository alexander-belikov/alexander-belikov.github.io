[{"authors":null,"categories":null,"content":"Alexander Belikov leads the Data Science team at Hello Watt, Paris.\n","date":1647907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1647907200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alexander-belikov.github.io/author/alexander-belikov/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alexander-belikov/","section":"authors","summary":"Alexander Belikov leads the Data Science team at Hello Watt, Paris.","tags":null,"title":"Alexander Belikov","type":"authors"},{"authors":["Alexander Belikov","Guillaume Matheron","Johan Sassi"],"categories":null,"content":"Supplementary notes can be added here, including code, math, and images. ","date":1647907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647907200,"objectID":"665a76ac9374d04cd9b9d52978f1b128","permalink":"https://alexander-belikov.github.io/publication/2022-hw-wh/","publishdate":"2022-03-22T00:00:00Z","relpermalink":"/publication/2022-hw-wh/","section":"publication","summary":"In this article we present an unsupervised low-frequency method aimed at detecting and disaggregating the power used by Cumulative Water Heaters (CWH) in residential homes. Our model circumvents the inherent difficulty of unsupervised signal disaggregation by using both the shape of a power spike and its time of occurrence to identify the contribution of CWH reliably. Indeed, many CHWs in France are configured to turn on automatically during off-peak hours only, and we are able to use this domain knowledge to aid peak identification despite the low sampling frequency. In order to test our model, we equipped a home with sensors to record the ground-truth consumption of a water heater. We then apply the model to a larger dataset of energy consumption of Hello Watt users consisting of one month of consumption data for 5k homes at 30-minute resolution. In this dataset we successfully identified CWHs in the majority of cases where consumers declared using them. The remaining part is likely due to possible misconfiguration of CWHs, since triggering them during off-peak hours requires specific wiring in the electrical panel of the house. Our model, despite its simplicity, offers promising applications: detection of mis-configured CWHs on off-peak contracts and slow performance degradation.","tags":["NILM"],"title":"Domain Knowledge Aids in Signal Disaggregation; the Example of the Cumulative Water Heater","type":"publication"},{"authors":["Alexander Belikov","Anant Matai"],"categories":null,"content":"We live in an era of accelerating data generation. The sizes of datasets keep growing and so does their structure. Despite the continuously growing capacity of computers real-world datasets surpass the limits of in-memory processing of even larger commercially available computers, and so data manipulation and analysis has to be aided by the use of databases. Across multiple domains, notably sociology, most interesting phenomena tend to resemble graphs rather than tables. Graph databases, compared to their more established SQL counterparts, offer multiple advantages, due to a more natural representation of the data. Instead of tables, where each row contains a record with potentially empty fields, in graph databases, entities are vertices and relations between them are represented by edges. A well-known join operation is represented as a loop over neighbouring vertices.\nIn this blogpost we describe how we set up MySQL and ArangoDB [1] databases on a subset of the Web of Science dataset [2] and designed and benchmarked sample queries during our tenure at Knowledge Lab [3].\nSample Dataset Web of Science (WoS) is one of the most complete academic datasets produced by Clarivate Analytics (previously by Thomson Reuters). As of 2017 it covered 12K journals and 160K conference proceedings. It contains publication records including authors, affiliations, references, abstracts etc. For our test we took a subset of available records published between 1971 and 1980. To properly highlight the case for graph databases application (and to simplify ingestion scripts) for each record we retrieved and cast the WoS dataset (originally in XML form) as csv-files containing certain relations:\n publications (publication metadata, contains journal information) institutions (affiliation data) contributors (author data) refs (citation data)  As a results we ended up with 5.87M unique publications, 12.3M rows in contributors, 68.7M rows in refs, 163K rows in institutions (affiliation data coverage seems to be incomplete).\nBelow are examples of different types of intermediate tables:\nPublications\n   wos_id issn title pubyear pubmonth language source     P1 J1 Title1 1960 1 English CHILD DEVELOPMENT   P2 J1 Title2 1962 1 English CHILD DEVELOPMENT   P3 J1 Title3 1963 1 English CHILD DEVELOPMENT    Contributors\n   wos_id position first_name last_name     P1 1 A L1   P1 2 A L2   P2 1 A L3    Institutions\n   wos_id organization country city     P1 O1 Switzerland Geneva   P2 O1 Switzerland Geneva   P3 O1 Switzerland Geneva    References\n   wos_id uid     P8 P1   P8 P2   P7 P4    MySQL setting CSV files were directly loaded into MySQL database and corresponding indices were created. MySQL was set up on a desktop with 64Gb of RAM. Ingestion took approximately 35 minutes.\nArangoDB setting We used ArangoDB version 3.4, hosted on an Ubuntu 16.04 with 16Gb of ram. For arangodb each type of csv file (blue) was projected on one or more vertex collections (green). From the point of view of a graph database each encodes not only a node in a vertex collection (potentially non-unique), but also an edge (hopefully unique). The inherent difficulty is to create edges between correct vertices. For that we decide which fields are going to uniquely describe a document from a vertex collection. The ingestion process took approximately 12 hours. While for the SQL test ingestion the tables were simply loaded into a MySQL database, for the graph database we upserted the vertices, i.e. for each vertex upsert we had to check if a vertex with the same index fields exists in the collection and retrieve its id if it did.\nOne of the crucial steps is to define the mappings of table columns onto field of vertex collection. Vertex collections contained the following fields (purple, with indices in boxed fields).\nIn order to insert edge instances, we first created edge collections as collections between vertex collections. Each document in an edge collection must contai two fields _to and _from, with corresponding values being the ids of the source and the target vertices.\nPlease note the edge connecting publication collection to itself, representing citations: both wos_id and uid have to be mapped to the same id of the publication collection.\nOur ingestion pipeline used pyarango package and arango query language aql. pyarango provides an interfaces to a limited number of functions to manage ArangoDB, so certain steps were executed as aql queries.\nDe-duplication of existing documents (documents are considered identical if defining fields, which were also indexed upon, are the same, grey boxes in the figure above) was part of the ingestion process (see upsert). The ingestion process took approximately 12 hours, of which citation ingestion took about 8.\nQueries  Calculate number of publications per journal for 1978 and return in descending order. Return 1000 most popular words (minus stop words) from all available titles. Find authors who changed their country more than twice. For a given publication p compute the ratio z of number of second order neighbors to first order neighbors in the directed network of citations. Count the number of times publications from journal j published in 1978 cite publications in journal j’ published in the previous 5 years. Given a subset of publications, compute the cardinality of the power set defined as papers cited by p, papers that are cited by papers cited by p and so on of order 5. Take the top 100 publications from Q4 with the highest ratio z.  Below we plot query times in seconds as a function of the size of the restricting subset, where applicable. Red, solid lines correspond to ArangoDB, blue, dashed lines and star markers - to MySQL.\nFor Q1, the limit is on the number of journals.\nFor Q2 - on the number of publications.\nFor Q3 - on the number of contributors.\nFor Q4 - on the number of publications.\nFor Q5 - on both the number of journals which are citing and which are being cited, so the for limit size N, the result is an NxN matrix. We note that Q5 essentially represents Eigenfactor [3] computation.\nFor Q6 - on the number of source publications.\nArango queries can be found wos_db_studies/run/queries\nSQL queries can be found wos_db_benchmark/benchmarking\nWhile our MySQL is less complete than that of ArangoDB (the results from Q2-Q4 were close to identical), where applicable the comparison reveals that MySQL shines where expected - in linear queries, such as looping over titles and does not do so well in graph-oriented tasks. Notably the results from ArangoDB for queries 3 and 4 surpass MySQL results by almost two orders of magnitude (!).\nConclusion The choice of schemas, the details of index definitions and computer hardware all contribute to the uncertainty of our results.\nNonetheless, we are able to conclude that:\n ArangoDB is a robust and well-supported database with excellent documentation (however, the learning curve might be steep compared to MySQL) Ingestion time for ArangoDB is greater than that for the MySQL counterpart ArangoDB beats MySQL on graph-oriented queries AQL is expressive with respect to graph-specific queries  References  https://www.arangodb.com/ https://en.wikipedia.org/wiki/Web_of_Science https://www.knowledgelab.org/ http://www.eigenfactor.org/  Acknowledgements Prepared with the help of Brendan Chambers.\n","date":1630713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"f72322e92cc908fb86f9638fa6b3a648","permalink":"https://alexander-belikov.github.io/post/2021-wos-arango-sql/","publishdate":"2021-09-04T00:00:00Z","relpermalink":"/post/2021-wos-arango-sql/","section":"post","summary":"We live in an era of accelerating data generation. The sizes of datasets keep growing and so does their structure. Despite the continuously growing capacity of computers real-world datasets surpass the limits of in-memory processing of even larger commercially available computers, and so data manipulation and analysis has to be aided by the use of databases.","tags":["Graph Database","ArangoDB","Web of Science","Knowledge Lab"],"title":"Pros and Cons of casting Web of Science into a graph database","type":"post"},{"authors":["Alexander Belikov"],"categories":null,"content":"","date":1621530000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621530000,"objectID":"590d2a77689d66dbc935614a2a5b6844","permalink":"https://alexander-belikov.github.io/talk/2021-arango/","publishdate":"2021-09-04T17:00:00Z","relpermalink":"/talk/2021-arango/","section":"talk","summary":"","tags":["graph database","ArangoDB"],"title":"ArangoDB : case for graphs","type":"talk"},{"authors":["Roselyne B Tchoua","Aswathy Ajith","Zhi Hong","Logan T Ward","Kyle Chard","Alexander Belikov","Debra J Audus","Shrayesh Patel","Juan J de Pablo","Ian T Foster"],"categories":[],"content":"","date":1615153341,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615153341,"objectID":"ba015a095e0727d84cdd6f94522aea53","permalink":"https://alexander-belikov.github.io/publication/2019-creating/","publishdate":"2019-06-07T22:42:21+01:00","relpermalink":"/publication/2019-creating/","section":"publication","summary":"Scientific Named Entity Referent Extraction is often more complicated than traditional Named Entity Recognition (NER). For example, in polymer science, chemical structure may be encoded in a variety of nonstandard naming conventions, and authors may refer to polymers with conventional names, commonly used names, labels (in lieu of longer names), synonyms, and acronyms. As a result, accurate scientific NER methods are often based on task-specific rules, which are difficult to develop and maintain, and are not easily generalized to other tasks and fields. Machine learning models require substantial expert-annotated data for training. Here we propose polyNER: a semi-automated system for efficient identification of scientific entities in text. PolyNER applies word embedding models to generate entity-rich corpora for productive expert labeling, and then uses the resulting labeled data to bootstrap a context-based word vector classifier. Evaluation on materials science publications shows that the polyNER approach enables improved precision or recall relative to a state-of-the-art chemical entity extraction system at a dramatically lower cost: it required just two hours of expert time, rather than extensive and expensive rule engineering, to achieve that result. This result highlights the potential for human-computer partnership for constructing domain-specific scientific NER systems. ","tags":["NER"],"title":"Creating training data for scientific named entity recognition with minimal human effort","type":"publication"},{"authors":["Alexander Belikov","Andrey Rzhetsky","James Evans"],"categories":[],"content":"","date":1615152859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615152859,"objectID":"6a1b4cda7019c4db76034bbb99e56107","permalink":"https://alexander-belikov.github.io/publication/2020-detecting/","publishdate":"2022-04-28T12:00:00+01:00","relpermalink":"/publication/2020-detecting/","section":"publication","summary":"The growth of published science in recent years has escalated the difficulty that human and algorithmic agents face in reasoning over prior knowledge to select the next experiment. This challenge is increased by uncertainty about the reproducibility of published findings. The availability of massive digital archives, machine reading, extraction tools and automated high-throughput experiments allows us to evaluate these challenges computationally at scale and identify novel opportunities to craft policies that accelerate scientific progress. Here we demonstrate a Bayesian calculus that enables positive prediction of robust scientific claims with findings extracted from published literature, weighted by scientific, social and institutional factors demonstrated to increase replicability. Illustrated with the case of gene regulatory interactions, our approach automatically estimates and counteracts sources of bias, revealing that scientifically focused but socially and institutionally diverse research activity is most likely to replicate. This results in updated certainty about the literature, which accurately predicts robust scientific facts on which new experiments should build. Our findings allow us to identify and evaluate policy recommendations for scientific institutions that may increase robust scientific knowledge, including sponsorship of increased diversity of and independence between investigations of any particular scientific phenomenon, and diversity of scientific phenomena investigated.","tags":["science of science","network"],"title":"Prediction of robust scientific facts from literature","type":"publication"},{"authors":["François Culière","Laetitia Leduc","Alexander Belikov"],"categories":null,"content":"Supplementary notes can be added here, including code, math, and images. ","date":1605052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605052800,"objectID":"d8ac1676191a93aeeffde00694be06fd","permalink":"https://alexander-belikov.github.io/publication/2020/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/2020/","section":"publication","summary":"Adoption of smart meters is a major milestone on the path of European transition to smart energy. The residential sector in France represents ≈35% of electricity consumption with ≈40% (INSEE) of households using electrical heating. The number of deployed smart meters Linky is expected to reach 35M in 2021. In this manuscript we present an analysis of 676 households with an observation period of at least 6 months, for which we have metadata, such as the year of construction and the type of heating and propose a Bayesian model of the electrical consumption conditioned on temperature that allows to disaggregate the heating component from the electrical load curve in an unsupervised manner. In essence the model is a mixture of piece-wise linear models, characterised by a temperature threshold, below which we allow a mixture of two modes to represent the latent state home/away.","tags":["NILM"],"title":"Bayesian model of electrical heating disaggregation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math  Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://alexander-belikov.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://alexander-belikov.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"}]