[{"authors":null,"categories":null,"content":"I am leading the Data Science team at Hello Watt, Paris. My research interest include research interests include graph neural networks, language modeling and knowledge graphs.\n","date":1651181659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1651181659,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"alexander-belikov.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"alexander-belikov.github.io/authors/admin/","section":"authors","summary":"I am leading the Data Science team at Hello Watt, Paris. My research interest include research interests include graph neural networks, language modeling and knowledge graphs.","tags":null,"title":"Alexander Belikov","type":"authors"},{"authors":["Alexander Belikov"],"categories":null,"content":"","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"4e7308c4517e88034e065c5831833f74","permalink":"alexander-belikov.github.io/event/quantifying-scienti%EF%AC%81c-discovery-to-improve-the-knowledge-of-facts/","publishdate":"2022-12-15T16:00:00Z","relpermalink":"alexander-belikov.github.io/event/quantifying-scienti%EF%AC%81c-discovery-to-improve-the-knowledge-of-facts/","section":"event","summary":"The ever-increasing amount of published academic results poses a challenge in interpretation and validation of these publications and rendering them to scientific facts. Despite the apparent lack of alignment between published claims and established facts, accounting for network structure enables predictive models that can assess the validity of published claims. Using pre-trained models on simulated alternative attention and local clustering distributions (which translates to modifications of funding policies) of academic publication we show that the overall knowledge of facts may be dramatically improved. We conclude by a discussion of applications of our methodology to other domains.","tags":["Metascience","Science of Science","Discovery Optimization"],"title":"Quantifying ScientiÔ¨Åc Discovery to Improve the Knowledge of Facts","type":"event"},{"authors":["Alexander Belikov"],"categories":null,"content":"","date":1662681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662681600,"objectID":"265391263e62e08ad36de4ecf64f9b78","permalink":"alexander-belikov.github.io/event/quantification-of-scientific-discovery-process-implies-better-science/","publishdate":"2022-09-09T16:00:00Z","relpermalink":"alexander-belikov.github.io/event/quantification-of-scientific-discovery-process-implies-better-science/","section":"event","summary":"The ever-increasing amount of published science poses a challenge in interpretation and validation of these publications and the formation of scientific facts. Despite the apparent lack of alignment between published claims and established facts, accounting for network structure enables predictive models that can assess the validity of published claims and render scientific facts, using the example of gene-gene interaction data.  Our simulations, based on pre-trained models, imply that the overall knowledge of facts can be improved by shifting the attention of the scientific community or modifying the funding policies. In conclusion, we review alternative approaches to optimization of the scientific discovery process and discuss alternative domains of application of our methodology.","tags":["Metascience","Science of Science","Discovery Optimization"],"title":"Quantification of Scientific Discovery Process Implies Better Science","type":"event"},{"authors":["Alexander Belikov"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"fc4c3e4c1901377e5532b6c6ac5b5b3c","permalink":"alexander-belikov.github.io/event/knowledge-graph-driven-discovery/","publishdate":"2022-09-09T16:00:00Z","relpermalink":"alexander-belikov.github.io/event/knowledge-graph-driven-discovery/","section":"event","summary":"A Gentle Introduction to Knowledge Graphs","tags":["Knowledge Graphs","Science of Science"],"title":"Knowledge Graph driven Discovery","type":"event"},{"authors":["Alexander Belikov","Andrey Rzhetsky","James Evans"],"categories":[],"content":"","date":1651181659,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651181659,"objectID":"6a1b4cda7019c4db76034bbb99e56107","permalink":"alexander-belikov.github.io/publication/2020-detecting/","publishdate":"2022-04-28T12:00:00+01:00","relpermalink":"alexander-belikov.github.io/publication/2020-detecting/","section":"publication","summary":"The growth of published science in recent years has escalated the difficulty that human and algorithmic agents face in reasoning over prior knowledge to select the next experiment. This challenge is increased by uncertainty about the reproducibility of published findings. The availability of massive digital archives, machine reading, extraction tools and automated high-throughput experiments allows us to evaluate these challenges computationally at scale and identify novel opportunities to craft policies that accelerate scientific progress. Here we demonstrate a Bayesian calculus that enables positive prediction of robust scientific claims with findings extracted from published literature, weighted by scientific, social and institutional factors demonstrated to increase replicability. Illustrated with the case of gene regulatory interactions, our approach automatically estimates and counteracts sources of bias, revealing that scientifically focused but socially and institutionally diverse research activity is most likely to replicate. This results in updated certainty about the literature, which accurately predicts robust scientific facts on which new experiments should build. Our findings allow us to identify and evaluate policy recommendations for scientific institutions that may increase robust scientific knowledge, including sponsorship of increased diversity of and independence between investigations of any particular scientific phenomenon, and diversity of scientific phenomena investigated.","tags":["Science of Science","Networks"],"title":"Prediction of robust scientific facts from literature","type":"publication"},{"authors":["Jamshid Sourati","Alexander Belikov","James Evans"],"categories":[],"content":"","date":1651100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651100400,"objectID":"15ad3fa955a96368867534ea483e25ad","permalink":"alexander-belikov.github.io/publication/2022-science/","publishdate":"2022-04-28T00:00:00+01:00","relpermalink":"alexander-belikov.github.io/publication/2022-science/","section":"publication","summary":"Science is an engine of innovation and economic growth and a pathway to prosperity for countries around the world. The increasing availability of scientific publications today poses a data-driven opportunity to better understand and improve science. Scientific publications contain data on the content of published research and metadata on the context that gave rise to that research. Here we discuss and demonstrate the power of constructing, archiving, and analyzing links between scientific data and metadata to construct massive computational observatories of and for modern science. We show how these can be constructed using modern graph databases, and suggest some methods of analysis with potential to unleash sustained value for science and society. These scientific observatories would allow us to diagnose the health of the scientific workforce and institutions, and track the rate of scientific advance. They could enable us to better guide science policy and build portfolios of supported research that balance our societal commitments to diverse participation and prosperity. Moreover, they could enable scientists to surf the deluge of published research to open the scientific frontier in directions that do not follow the current, but open up new views and opportunities for others to follow. Linked scientific data can also enable the construction of artificial intelligence agents designed to complement the disciplinary focus of human scientific attention by proposing possibilities overlooked or underfunded by contemporary scientific institutions. Finally, we argue for the importance of ongoing political and legal support for the promotion of open, linked data to facilitate widespread benefit.","tags":["Science of Science","Networks"],"title":"Data on How Science Is Made Can Make Science Better","type":"publication"},{"authors":["Alexander Belikov","Guillaume Matheron","Johan Sassi"],"categories":null,"content":"Supplementary notes can be added here, including code, math, and images. ","date":1647907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647907200,"objectID":"665a76ac9374d04cd9b9d52978f1b128","permalink":"alexander-belikov.github.io/publication/2022-hw-wh/","publishdate":"2022-03-22T00:00:00Z","relpermalink":"alexander-belikov.github.io/publication/2022-hw-wh/","section":"publication","summary":"In this article we present an unsupervised low-frequency method aimed at detecting and disaggregating the power used by Cumulative Water Heaters (CWH) in residential homes. Our model circumvents the inherent difficulty of unsupervised signal disaggregation by using both the shape of a power spike and its temporal pattern to identify the contribution of CWH reliably. Indeed, many CHWs in France are configured to turn on automatically during off-peak hours only, and we are able to use this domain knowledge to aid peak identification despite the low sampling frequency.\nIn order to test our model, we equipped a home with sensors to record the ground truth consumption of a water heater. We then apply the model to a larger dataset of energy consumption of Hello Watt users consisting of one month of consumption data for 5 k homes at 30-min resolution. In this dataset we successfully identified CWHs in 66.5% of cases where consumers declared using them. Inability of our model to identify CWHs in the consumption signal in the remaining cases is likely due to possible misconfiguration of CWHs, since triggering them during off-peak hours requires specific wiring in the electrical panel of the house. Our model, despite its simplicity, offers promising applications: detection of mis-configured CWHs on off-peak contracts and slow performance degradation.","tags":["NILM"],"title":"Domain Knowledge Aids in Signal Disaggregation; the Example of the Cumulative Water Heater","type":"publication"},{"authors":["Alexander Belikov","Anant Matai"],"categories":null,"content":"We live in an era of accelerating data generation. The sizes of datasets keep growing and so does their structure. Despite the continuously growing capacity of computers real-world datasets surpass the limits of in-memory processing of even larger commercially available computers, and so data manipulation and analysis has to be aided by the use of databases. Across multiple domains, notably sociology, most interesting phenomena tend to resemble graphs rather than tables. Graph databases, compared to their more established SQL counterparts, offer multiple advantages, due to a more natural representation of the data. Instead of tables, where each row contains a record with potentially empty fields, in graph databases, entities are vertices and relations between them are represented by edges. A well-known join operation is represented as a loop over neighbouring vertices.\nIn this blogpost we describe how we set up MySQL and ArangoDB [1] databases on a subset of the Web of Science dataset [2] and designed and benchmarked sample queries during our tenure at Knowledge Lab [3].\nSample Dataset Web of Science (WoS) is one of the most complete academic datasets produced by Clarivate Analytics (previously by Thomson Reuters). As of 2017 it covered 12K journals and 160K conference proceedings. It contains publication records including authors, affiliations, references, abstracts etc. For our test we took a subset of available records published between 1971 and 1980. To properly highlight the case for graph databases application (and to simplify ingestion scripts) for each record we retrieved and cast the WoS dataset (originally in XML form) as csv-files containing certain relations:\npublications (publication metadata, contains journal information) institutions (affiliation data) contributors (author data) refs (citation data) As a results we ended up with 5.87M unique publications, 12.3M rows in contributors, 68.7M rows in refs, 163K rows in institutions (affiliation data coverage seems to be incomplete).\nBelow are examples of different types of intermediate tables:\nPublications\nwos_id issn title pubyear pubmonth language source P1 J1 Title1 1960 1 English CHILD DEVELOPMENT P2 J1 Title2 1962 1 English CHILD DEVELOPMENT P3 J1 Title3 1963 1 English CHILD DEVELOPMENT Contributors\nwos_id position first_name last_name P1 1 A L1 P1 2 A L2 P2 1 A L3 Institutions\nwos_id organization country city P1 O1 Switzerland Geneva P2 O1 Switzerland Geneva P3 O1 Switzerland Geneva References\nwos_id uid P8 P1 P8 P2 P7 P4 MySQL setting CSV files were directly loaded into MySQL database and corresponding indices were created. MySQL was set up on a desktop with 64Gb of RAM. Ingestion took approximately 35 minutes.\nArangoDB setting We used ArangoDB version 3.4, hosted on an Ubuntu 16.04 with 16Gb of ram. For arangodb each type of csv file (blue) was projected on one or more vertex collections (green). From the point of view of a graph database each encodes not only a node in a vertex collection (potentially non-unique), but also an edge (hopefully unique). The inherent difficulty is to create edges between correct vertices. For that we decide which fields are going to uniquely describe a document from a vertex collection. The ingestion process took approximately 12 hours. While for the SQL test ingestion the tables were simply loaded into a MySQL database, for the graph database we upserted the vertices, i.e. for each vertex upsert we had to check if a vertex with the same index fields exists in the collection and retrieve its id if it did.\nOne of the crucial steps is to define the mappings of table columns onto field of vertex collection. Vertex collections contained the following fields (purple, with indices in boxed fields).\nIn order to insert edge instances, we first created edge collections as collections between vertex collections. Each document in an edge collection must contai two fields _to and _from, with corresponding values being the ids of the source and the target vertices.\nPlease note the edge connecting publication collection to itself, representing citations: both wos_id and uid have to be mapped to the same id of the publication collection.\nOur ingestion pipeline used pyarango package and arango query language aql. pyarango provides an interfaces to a limited number of functions to manage ArangoDB, so certain steps were executed as aql queries.\nDe-duplication of existing documents (documents are considered identical if defining fields, which were also indexed upon, are the same, grey boxes in the figure above) was part of the ingestion process (see upsert). The ingestion process took approximately 12 hours, of which citation ingestion took about 8.\nQueries Calculate number of publications per journal for 1978 and return in descending order. Return 1000 most popular words (minus stop words) from all available titles. Find authors who changed their country more than twice. For a given publication p compute the ratio z of number ‚Ä¶","date":1630713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"f72322e92cc908fb86f9638fa6b3a648","permalink":"alexander-belikov.github.io/post/2021-wos-arango-sql/","publishdate":"2021-09-04T00:00:00Z","relpermalink":"alexander-belikov.github.io/post/2021-wos-arango-sql/","section":"post","summary":"We live in an era of accelerating data generation. The sizes of datasets keep growing and so does their structure. Despite the continuously growing capacity of computers real-world datasets surpass the limits of in-memory processing of even larger commercially available computers, and so data manipulation and analysis has to be aided by the use of databases.","tags":["Graph Database","ArangoDB","Web of Science","Knowledge Lab"],"title":"Pros and Cons of casting Web of Science into a graph database","type":"post"},{"authors":["Alexander Belikov"],"categories":null,"content":"","date":162153e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":162153e4,"objectID":"fa2526359419c1cd1583389d3929f594","permalink":"alexander-belikov.github.io/event/arangodb-case-for-graphs/","publishdate":"2021-09-04T17:00:00Z","relpermalink":"alexander-belikov.github.io/event/arangodb-case-for-graphs/","section":"event","summary":"In this talk we discuss utility of graph databases using the example of ArangoDB.","tags":["graph database","ArangoDB"],"title":"ArangoDB : case for graphs","type":"event"},{"authors":["Fran√ßois Culi√®re","Laetitia Leduc","Alexander Belikov"],"categories":null,"content":"Supplementary notes can be added here, including code, math, and images. ","date":1605052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605052800,"objectID":"658cf72168d38dc473eac6e784399ce9","permalink":"alexander-belikov.github.io/publication/2020-hw-heating/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"alexander-belikov.github.io/publication/2020-hw-heating/","section":"publication","summary":"Adoption of smart meters is a major milestone on the path of European transition to smart energy. The residential sector in France represents ‚âà35% of electricity consumption with ‚âà40% (INSEE) of households using electrical heating. The number of deployed smart meters Linky is expected to reach 35M in 2021. In this manuscript we present an analysis of 676 households with an observation period of at least 6 months, for which we have metadata, such as the year of construction and the type of heating and propose a Bayesian model of the electrical consumption conditioned on temperature that allows to disaggregate the heating component from the electrical load curve in an unsupervised manner. In essence the model is a mixture of piece-wise linear models, characterised by a temperature threshold, below which we allow a mixture of two modes to represent the latent state home/away.","tags":["NILM"],"title":"Bayesian model of electrical heating disaggregation","type":"publication"},{"authors":["Roselyne B Tchoua","Aswathy Ajith","Zhi Hong","Logan T Ward","Kyle Chard","Alexander Belikov","Debra J Audus","Shrayesh Patel","Juan J de Pablo","Ian T Foster"],"categories":[],"content":"","date":1559943741,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559943741,"objectID":"ba015a095e0727d84cdd6f94522aea53","permalink":"alexander-belikov.github.io/publication/2019-creating/","publishdate":"2019-06-07T22:42:21+01:00","relpermalink":"alexander-belikov.github.io/publication/2019-creating/","section":"publication","summary":"Scientific Named Entity Referent Extraction is often more complicated than traditional Named Entity Recognition (NER). For example, in polymer science, chemical structure may be encoded in a variety of nonstandard naming conventions, and authors may refer to polymers with conventional names, commonly used names, labels (in lieu of longer names), synonyms, and acronyms. As a result, accurate scientific NER methods are often based on task-specific rules, which are difficult to develop and maintain, and are not easily generalized to other tasks and fields. Machine learning models require substantial expert-annotated data for training. Here we propose polyNER: a semi-automated system for efficient identification of scientific entities in text. PolyNER applies word embedding models to generate entity-rich corpora for productive expert labeling, and then uses the resulting labeled data to bootstrap a context-based word vector classifier. Evaluation on materials science publications shows that the polyNER approach enables improved precision or recall relative to a state-of-the-art chemical entity extraction system at a dramatically lower cost: it required just two hours of expert time, rather than extensive and expensive rule engineering, to achieve that result. This result highlights the potential for human-computer partnership for constructing domain-specific scientific NER systems. ","tags":["NER"],"title":"Creating training data for scientific named entity recognition with minimal human effort","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"alexander-belikov.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"alexander-belikov.github.io/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"alexander-belikov.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"alexander-belikov.github.io/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"alexander-belikov.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"alexander-belikov.github.io/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]