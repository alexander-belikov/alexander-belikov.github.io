<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Knowledge Lab | Alexander Belikov</title><link>https://alexander-belikov.github.io/tag/knowledge-lab/</link><atom:link href="https://alexander-belikov.github.io/tag/knowledge-lab/index.xml" rel="self" type="application/rss+xml"/><description>Knowledge Lab</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 04 Sep 2021 00:00:00 +0000</lastBuildDate><image><url>https://alexander-belikov.github.io/media/icon_huf49b158b2681d286a60137da38ca7574_154815_512x512_fill_lanczos_center_3.png</url><title>Knowledge Lab</title><link>https://alexander-belikov.github.io/tag/knowledge-lab/</link></image><item><title>Pros and Cons of casting Web of Science into a graph database</title><link>https://alexander-belikov.github.io/post/2021-wos-arango-sql/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://alexander-belikov.github.io/post/2021-wos-arango-sql/</guid><description>&lt;p>We live in an era of accelerating data generation. The sizes of datasets keep growing and so does their structure. Despite the continuously growing capacity of computers real-world datasets surpass the limits of in-memory processing of even larger commercially available computers, and so data manipulation and analysis has to be aided by the use of databases. Across multiple domains, notably sociology, most interesting phenomena tend to resemble graphs rather than tables.
Graph databases, compared to their more established SQL counterparts, offer multiple advantages, due to a more natural representation of the data. Instead of tables, where each row contains a record with potentially empty fields, in graph databases, entities are vertices and relations between them are represented by edges. A well-known &lt;em>join&lt;/em> operation is represented as a loop over neighbouring vertices.&lt;/p>
&lt;p>In this blogpost we describe how we set up MySQL and ArangoDB [1] databases on a subset of the Web of Science dataset [2] and designed and benchmarked sample queries during our tenure at Knowledge Lab [3].&lt;/p>
&lt;h2 id="sample-dataset">Sample Dataset&lt;/h2>
&lt;p>Web of Science (WoS) is one of the most complete academic datasets produced by Clarivate Analytics (previously by Thomson Reuters). As of 2017 it covered 12K journals and 160K conference proceedings. It contains publication records including authors, affiliations, references, abstracts etc.
For our test we took a subset of available records published between 1971 and 1980.
To properly highlight the case for graph databases application (and to simplify ingestion scripts) for each record we retrieved and cast the WoS dataset (originally in XML form) as csv-files containing certain relations:&lt;/p>
&lt;ul>
&lt;li>publications (publication metadata, contains journal information)&lt;/li>
&lt;li>institutions (affiliation data)&lt;/li>
&lt;li>contributors (author data)&lt;/li>
&lt;li>refs (citation data)&lt;/li>
&lt;/ul>
&lt;p>As a results we ended up with 5.87M unique publications, 12.3M rows in contributors, 68.7M rows in refs, 163K rows in institutions (affiliation data coverage seems to be incomplete).&lt;/p>
&lt;p>Below are examples of different types of intermediate tables:&lt;/p>
&lt;p>&lt;strong>Publications&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">wos_id&lt;/th>
&lt;th style="text-align:left">issn&lt;/th>
&lt;th style="text-align:left">title&lt;/th>
&lt;th style="text-align:right">pubyear&lt;/th>
&lt;th style="text-align:right">pubmonth&lt;/th>
&lt;th style="text-align:left">language&lt;/th>
&lt;th style="text-align:left">source&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">P1&lt;/td>
&lt;td style="text-align:left">J1&lt;/td>
&lt;td style="text-align:left">Title1&lt;/td>
&lt;td style="text-align:right">1960&lt;/td>
&lt;td style="text-align:right">1&lt;/td>
&lt;td style="text-align:left">English&lt;/td>
&lt;td style="text-align:left">CHILD DEVELOPMENT&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P2&lt;/td>
&lt;td style="text-align:left">J1&lt;/td>
&lt;td style="text-align:left">Title2&lt;/td>
&lt;td style="text-align:right">1962&lt;/td>
&lt;td style="text-align:right">1&lt;/td>
&lt;td style="text-align:left">English&lt;/td>
&lt;td style="text-align:left">CHILD DEVELOPMENT&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P3&lt;/td>
&lt;td style="text-align:left">J1&lt;/td>
&lt;td style="text-align:left">Title3&lt;/td>
&lt;td style="text-align:right">1963&lt;/td>
&lt;td style="text-align:right">1&lt;/td>
&lt;td style="text-align:left">English&lt;/td>
&lt;td style="text-align:left">CHILD DEVELOPMENT&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Contributors&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">wos_id&lt;/th>
&lt;th style="text-align:right">position&lt;/th>
&lt;th style="text-align:left">first_name&lt;/th>
&lt;th style="text-align:left">last_name&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">P1&lt;/td>
&lt;td style="text-align:right">1&lt;/td>
&lt;td style="text-align:left">A&lt;/td>
&lt;td style="text-align:left">L1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P1&lt;/td>
&lt;td style="text-align:right">2&lt;/td>
&lt;td style="text-align:left">A&lt;/td>
&lt;td style="text-align:left">L2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P2&lt;/td>
&lt;td style="text-align:right">1&lt;/td>
&lt;td style="text-align:left">A&lt;/td>
&lt;td style="text-align:left">L3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Institutions&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">wos_id&lt;/th>
&lt;th style="text-align:left">organization&lt;/th>
&lt;th style="text-align:left">country&lt;/th>
&lt;th style="text-align:left">city&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">P1&lt;/td>
&lt;td style="text-align:left">O1&lt;/td>
&lt;td style="text-align:left">Switzerland&lt;/td>
&lt;td style="text-align:left">Geneva&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P2&lt;/td>
&lt;td style="text-align:left">O1&lt;/td>
&lt;td style="text-align:left">Switzerland&lt;/td>
&lt;td style="text-align:left">Geneva&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P3&lt;/td>
&lt;td style="text-align:left">O1&lt;/td>
&lt;td style="text-align:left">Switzerland&lt;/td>
&lt;td style="text-align:left">Geneva&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">wos_id&lt;/th>
&lt;th style="text-align:left">uid&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">P8&lt;/td>
&lt;td style="text-align:left">P1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P8&lt;/td>
&lt;td style="text-align:left">P2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">P7&lt;/td>
&lt;td style="text-align:left">P4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="mysql-setting">MySQL setting&lt;/h3>
&lt;p>CSV files were directly loaded into MySQL database and corresponding indices were created.
MySQL was set up on a desktop with 64Gb of RAM. Ingestion took approximately 35 minutes.&lt;/p>
&lt;h3 id="arangodb-setting">ArangoDB setting&lt;/h3>
&lt;p>We used ArangoDB version 3.4, hosted on an Ubuntu 16.04 with 16Gb of ram.
For arangodb each type of csv file (blue) was projected on one or more vertex collections (green). From the point of view of a graph database each encodes not only a node in a vertex collection (potentially non-unique), but also an edge (hopefully unique). The inherent difficulty is to create edges between correct vertices. For that we decide which fields are going to uniquely describe a document from a vertex collection. The ingestion process took approximately 12 hours. While for the SQL test ingestion the tables were simply loaded into a MySQL database, for the graph database we upserted the vertices, i.e. for each vertex &lt;em>upsert&lt;/em> we had to check if a vertex with the same index fields exists in the collection and retrieve its &lt;em>id&lt;/em> if it did.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/wos_csv_source2vc.png" alt="source2vc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>One of the crucial steps is to define the mappings of table columns onto field of vertex collection.
Vertex collections contained the following fields (purple, with indices in boxed fields).&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/wos_csv_vc2fields.png" alt="source2vc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>In order to insert edge instances, we first created edge collections as collections between vertex collections. Each document in an edge collection must contai two fields &lt;em>_to&lt;/em> and &lt;em>_from&lt;/em>, with corresponding values being the ids of the source and the target vertices.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/wos_csv_vc2vc.png" alt="source2vc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Please note the edge connecting &lt;em>publication&lt;/em> collection to itself, representing citations: both &lt;em>wos_id&lt;/em> and &lt;em>uid&lt;/em> have to be mapped to the same &lt;em>id&lt;/em> of the publication collection.&lt;/p>
&lt;p>Our ingestion pipeline used &lt;code>pyarango&lt;/code> package and arango query language &lt;code>aql&lt;/code>.
&lt;code>pyarango&lt;/code> provides an interfaces to a limited number of functions to manage ArangoDB, so certain steps were executed as &lt;code>aql&lt;/code> queries.&lt;/p>
&lt;p>De-duplication of existing documents (documents are considered identical if defining fields, which were also indexed upon, are the same, grey boxes in the figure above) was part of the ingestion process (see upsert).
The ingestion process took approximately 12 hours, of which citation ingestion took about 8.&lt;/p>
&lt;h3 id="queries">Queries&lt;/h3>
&lt;ol>
&lt;li>Calculate number of publications per journal for 1978 and return in descending order.&lt;/li>
&lt;li>Return 1000 most popular words (minus stop words) from all available titles.&lt;/li>
&lt;li>Find authors who changed their country more than twice.&lt;/li>
&lt;li>For a given publication &lt;em>p&lt;/em> compute the ratio &lt;em>z&lt;/em> of number of second order neighbors to first order neighbors in the directed network of citations.&lt;/li>
&lt;li>Count the number of times publications from journal &lt;em>j&lt;/em> published in 1978 cite publications in journal &lt;em>jâ€™&lt;/em> published in the previous 5 years.&lt;/li>
&lt;li>Given a subset of publications, compute the cardinality of the power set defined as papers cited by &lt;em>p&lt;/em>, papers that are cited by papers cited by &lt;em>p&lt;/em> and so on of order 5. Take the top 100 publications from Q4 with the highest ratio &lt;em>z&lt;/em>.&lt;/li>
&lt;/ol>
&lt;p>Below we plot query times in seconds as a function of the size of the restricting subset, where applicable. Red, solid lines correspond to ArangoDB, blue, dashed lines and star markers - to MySQL.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/queries_plot_times.png" alt="source2vc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>For Q1, the limit is on the number of journals.&lt;/p>
&lt;p>For Q2 - on the number of publications.&lt;/p>
&lt;p>For Q3 - on the number of contributors.&lt;/p>
&lt;p>For Q4 - on the number of publications.&lt;/p>
&lt;p>For Q5 - on both the number of journals which are citing and which are being cited, so the for limit size N, the result is an NxN matrix. We note that Q5 essentially represents Eigenfactor [3] computation.&lt;/p>
&lt;p>For Q6 - on the number of source publications.&lt;/p>
&lt;p>Arango queries can be found &lt;a href="https://github.com/alexander-belikov/wos_db_studies/blob/master/run/queries.py" target="_blank" rel="noopener">wos_db_studies/run/queries&lt;/a>&lt;/p>
&lt;p>SQL queries can be found
&lt;a href="https://github.com/brendanchambers/wos_db_benchmark/tree/master/benchmarking" target="_blank" rel="noopener">wos_db_benchmark/benchmarking&lt;/a>&lt;/p>
&lt;p>While our MySQL is less complete than that of ArangoDB (the results from Q2-Q4 were close to identical), where applicable the comparison reveals that MySQL shines where expected - in linear queries, such as looping over titles and does not do so well in graph-oriented tasks. Notably the results from ArangoDB for queries 3 and 4 surpass MySQL results by almost two orders of magnitude (!).&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>The choice of schemas, the details of index definitions and computer hardware all contribute to the uncertainty of our results.&lt;br>
Nonetheless, we are able to conclude that:&lt;/p>
&lt;ul>
&lt;li>ArangoDB is a robust and well-supported database with excellent documentation (however, the learning curve might be steep compared to MySQL)&lt;/li>
&lt;li>Ingestion time for ArangoDB is greater than that for the MySQL counterpart&lt;/li>
&lt;li>ArangoDB beats MySQL on graph-oriented queries&lt;/li>
&lt;li>&lt;em>AQL&lt;/em> is expressive with respect to graph-specific queries&lt;/li>
&lt;/ul>
&lt;h3 id="references">References&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="https://www.arangodb.com/" target="_blank" rel="noopener">https://www.arangodb.com/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Web_of_Science" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Web_of_Science&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.knowledgelab.org/" target="_blank" rel="noopener">https://www.knowledgelab.org/&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.eigenfactor.org/" target="_blank" rel="noopener">http://www.eigenfactor.org/&lt;/a>&lt;/li>
&lt;/ol>
&lt;h3 id="acknowledgements">Acknowledgements&lt;/h3>
&lt;p>Prepared with the help of Brendan Chambers.&lt;/p></description></item></channel></rss>